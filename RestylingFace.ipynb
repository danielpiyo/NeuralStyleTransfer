{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhMIn0qVCB1wRjBDmPqDjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielpiyo/NeuralStyleTransfer/blob/main/RestylingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvz4Uj1Ztdjj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizer\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import save_image\n",
        "from torch.optim import Adam\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.vgg19(pretrained=True).features\n",
        "# print(model)"
      ],
      "metadata": {
        "id": "5lqIqecDzJ63"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egQ5Iyoz-0zi",
        "outputId": "14924826-03bc-46f5-c8ff-32335c111a10"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_image_path = \"/content/drive/MyDrive/modigliani_style/modigliani_image.jpg\"\n",
        "input_image_path = \"/content/drive/MyDrive/modigliani_style/input_image.jpg\"\n",
        "\n",
        "# Define the paths to the input and output images\n",
        "output_image_path = '/content/drive/MyDrive/modigliani_style/output_modigliani_style_image.jpg'"
      ],
      "metadata": {
        "id": "RuJqYGnM_Anw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "      super(VGG, self).__init__()\n",
        "      self.chosen_features = ['0', '5', '10', '19', '28']\n",
        "      self.model = models.vgg19(pretrained = True).features[:29]\n",
        "  def forward(self, x):\n",
        "    features = []\n",
        "    for llayer_num, layer in enumerate(self.model):\n",
        "      x = layer(x)\n",
        "      if str(llayer_num) in self.chosen_features:\n",
        "        features.append(x)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "SMJ-KmGazeTE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCz9I4Gn-0EG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "image_size = 356\n",
        "loader = transforms.Compose([transforms.Resize((image_size, image_size)), transforms.ToTensor()])\n"
      ],
      "metadata": {
        "id": "Sq1-lLmZ3LCk"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_name):\n",
        "  image = Image.open(image_name)\n",
        "  image = loader(image).unsqueeze(0)\n",
        "  return image.to(device)"
      ],
      "metadata": {
        "id": "6h-XGZO62u0Y"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_image = load_image(input_image_path)\n",
        "style_image = load_image(style_image_path)\n",
        "# print(original_image)\n",
        "# model = VGG.to(device).eval()\n",
        "model = VGG().to(device).eval()\n",
        "# generated = torch.randn(original_image, device = device, requires_grad=True)\n",
        "# generated_image = original_image.clone().requires_grad(True)\n",
        "# generated_image = torch.Tensor(original_image).clone().requires_grad_(True)\n",
        "generated_image = torch.Tensor(original_image[:,:3,:,:]).clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "X92DnLVt34h8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3a0d5d-124c-4da3-96ac-3901d1f6c9bc"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "total_steps = 6000\n",
        "learning_rate = 0.01\n",
        "alpha = 1\n",
        "beta = 0.01\n",
        "# optimizer = optimizer.Adam([generated_image],lr=learning_rate )\n",
        "optimizer = Adam([generated_image], lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "LS2dTEkE4oEL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "original_image = original_image[:, :3, :, :]\n",
        "for step in range(total_steps):\n",
        "  generated_features = model(generated_image)\n",
        "  original_image_features = model(original_image)\n",
        "  style_features = model(style_image)\n",
        "\n",
        "  style_loss = original_loss = 0\n",
        "\n",
        "  for gen_feature, orig_feature, style_feature in zip(\n",
        "      generated_features, original_image_features, style_features):\n",
        "    batch_size, channel, height, width = gen_feature.shape\n",
        "    original_loss += torch.mean((gen_feature - orig_feature) **2)\n",
        "\n",
        "    # compute gram Matrix\n",
        "    G = gen_feature.view(channel, height*width).mm(gen_feature.view(channel, height*width).t())\n",
        "    A = style_feature.view(channel, height*width).mm(style_feature.view(channel, height*width).t())\n",
        "\n",
        "    style_loss += torch.mean((G-A)**2)\n",
        "\n",
        "  total_loss = alpha*original_loss + beta*style_loss\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if step %200 == 0:\n",
        "    print('Iteration %d completed' % step)\n",
        "    print(total_loss)\n",
        "    save_image(generated_image, output_image_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krmhiR405gOP",
        "outputId": "e7c1f63a-5497-46c5-d74a-345dc94d9d92"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 completed\n",
            "tensor(340352.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 200 completed\n",
            "tensor(12991.7480, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 400 completed\n",
            "tensor(8076.8159, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 600 completed\n",
            "tensor(5576.0225, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 800 completed\n",
            "tensor(4129.9985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 1000 completed\n",
            "tensor(3542.6001, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 1200 completed\n",
            "tensor(2902.7661, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 1400 completed\n",
            "tensor(2524.3901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 1600 completed\n",
            "tensor(2259.7920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 1800 completed\n",
            "tensor(2056.9226, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 2000 completed\n",
            "tensor(1905.9907, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 2200 completed\n",
            "tensor(1783.6993, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 2400 completed\n",
            "tensor(2435.7981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 2600 completed\n",
            "tensor(1716.5885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 2800 completed\n",
            "tensor(1565.5894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 3000 completed\n",
            "tensor(1467.9462, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 3200 completed\n",
            "tensor(1403.9745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 3400 completed\n",
            "tensor(1360.6869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 3600 completed\n",
            "tensor(1292.5251, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 3800 completed\n",
            "tensor(1271.4885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 4000 completed\n",
            "tensor(1891.0105, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 4200 completed\n",
            "tensor(1380.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 4400 completed\n",
            "tensor(1276.0444, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 4600 completed\n",
            "tensor(1205.9404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 4800 completed\n",
            "tensor(1165.1736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 5000 completed\n",
            "tensor(1135.9661, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 5200 completed\n",
            "tensor(1143.3429, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 5400 completed\n",
            "tensor(1255.4808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 5600 completed\n",
            "tensor(1404.4156, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Iteration 5800 completed\n",
            "tensor(1195.3356, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTaYFrvj_cf9"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}